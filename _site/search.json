[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog.\nOn this site, I shall be posting about different aspects of my work, life and hobbies that I feel will help others going through similar things on their own life journeys. Although it may be heavily biased towards my learnings on data and analytics, it will also serve as a bit of a journal helping me to put my thoughts on a “piece of paper”.\nDo enjoy the works and if you want to link up, hit me up on Bluesky!"
  },
  {
    "objectID": "posts/tidy-data/index.html",
    "href": "posts/tidy-data/index.html",
    "title": "What is tidy data?",
    "section": "",
    "text": "Tidy data is a philosophy often used in the data analysis circles to help structure data before using it for analysis. Although getting it into this form is a lot of hard work, it is akin to sharpening the axe before cutting down a tree - it may be a lot of work to get the tool (data) in the right state but it pays off immensely once the cutting (analysis) starts.\nIn the R community, putting your data in a tidy state allows one to leverage on the tidyverse packages that are created to operate on data complying with this philosophy. This philosophy, coupled with packages that comply with it, makes analysis such a joy.\nThis article is focused on helping you understand what it means to have data in a tidy state."
  },
  {
    "objectID": "posts/tidy-data/index.html#background",
    "href": "posts/tidy-data/index.html#background",
    "title": "What is tidy data?",
    "section": "",
    "text": "Tidy data is a philosophy often used in the data analysis circles to help structure data before using it for analysis. Although getting it into this form is a lot of hard work, it is akin to sharpening the axe before cutting down a tree - it may be a lot of work to get the tool (data) in the right state but it pays off immensely once the cutting (analysis) starts.\nIn the R community, putting your data in a tidy state allows one to leverage on the tidyverse packages that are created to operate on data complying with this philosophy. This philosophy, coupled with packages that comply with it, makes analysis such a joy.\nThis article is focused on helping you understand what it means to have data in a tidy state."
  },
  {
    "objectID": "posts/tidy-data/index.html#principles-behind-tidy-data",
    "href": "posts/tidy-data/index.html#principles-behind-tidy-data",
    "title": "What is tidy data?",
    "section": "Principles behind tidy data",
    "text": "Principles behind tidy data\nData can be represented in several ways. Consider a simple dataset that shows a table of Series with their release date, genre, IMDB rating and director.\nTable 1 might looks like the following:\n\n\n\n\n\n\n\n\nSeries_Title\nRelease\nGenre\nIMDB_Rating\nDirector\n\n\n\n\nThe Shawshank Redemption\n1994\nDrama\n9.3\nFrank Darabont\n\n\nThe Godfather\n1972\nCrime\n9.2\nFrancis Ford Coppola\n\n\nThe Dark Knight\n2008\nAction\n9.0\nChristopher Nolan\n\n\nThe Godfather: Part II\n1974\nCrime\n9.0\nFrancis Ford Coppola\n\n\n12 Angry Men\n1957\nCrime\n9.0\nSidney Lumet\n\n\nThe Lord of the Rings: The Return of the King\n2003\nAction\n8.9\nPeter Jackson\n\n\nPulp Fiction\n1994\nCrime\n8.9\nQuentin Tarantino\n\n\nSchindler's List\n1993\nBiography\n8.9\nSteven Spielberg\n\n\nInception\n2010\nAction\n8.8\nChristopher Nolan\n\n\nFight Club\n1999\nDrama\n8.8\nDavid Fincher\n\n\n\n\n\n\n\nTable 2, containing the same data, might look like this:\n\n\n\n\n\n\n\n\nRelease\nGenre\nIMDB_Rating\nDirector\nSeries_Title_Rating\n\n\n\n\n1994\nDrama\n9.3\nFrank Darabont\nThe Shawshank Redemption , 9.3\n\n\n1972\nCrime\n9.2\nFrancis Ford Coppola\nThe Godfather , 9.2\n\n\n2008\nAction\n9.0\nChristopher Nolan\nThe Dark Knight , 9\n\n\n1974\nCrime\n9.0\nFrancis Ford Coppola\nThe Godfather: Part II , 9\n\n\n1957\nCrime\n9.0\nSidney Lumet\n12 Angry Men , 9\n\n\n2003\nAction\n8.9\nPeter Jackson\nThe Lord of the Rings: The Return of the King , 8.9\n\n\n1994\nCrime\n8.9\nQuentin Tarantino\nPulp Fiction , 8.9\n\n\n1993\nBiography\n8.9\nSteven Spielberg\nSchindler's List , 8.9\n\n\n2010\nAction\n8.8\nChristopher Nolan\nInception , 8.8\n\n\n1999\nDrama\n8.8\nDavid Fincher\nFight Club , 8.8\n\n\n\n\n\n\n\nOr Table 3 could be structured like this:\n\n\n\n\n\n\n\n\nSeries\nDetails\n\n\n\n\nThe Shawshank Redemption\nFrank Darabont , Drama , 1994 , 9.3\n\n\nThe Godfather\nFrancis Ford Coppola , Crime , 1972 , 9.2\n\n\nThe Dark Knight\nChristopher Nolan , Action , 2008 , 9\n\n\nThe Godfather: Part II\nFrancis Ford Coppola , Crime , 1974 , 9\n\n\n12 Angry Men\nSidney Lumet , Crime , 1957 , 9\n\n\nThe Lord of the Rings: The Return of the King\nPeter Jackson , Action , 2003 , 8.9\n\n\nPulp Fiction\nQuentin Tarantino , Crime , 1994 , 8.9\n\n\nSchindler's List\nSteven Spielberg , Biography , 1993 , 8.9\n\n\nInception\nChristopher Nolan , Action , 2010 , 8.8\n\n\nFight Club\nDavid Fincher , Drama , 1999 , 8.8\n\n\n\n\n\n\n\nIn all these three instances, the data representation is perfectly valid but each one is harder to use in analysis (particularly the third table!)\nThe first table is the easiest to use as it follows the three tidy principles:\n\nEach column is a variable.\nEach row is an observation.\nEach cell (intersection between the column and row) is a SINGLE value.\n\n\n\n\nA tidy data representation\n\n\nThese simple principles allows for data to be stored consistently, and allows for R to leverage on its vectorised approach processing data, i.e., a tidy function can perform the same transformation along a variable consistently if the data within that variable is the same.\nFor example, converting the IMDB_rating variable in the first table to a percentage would be as simple as multiplying the variable by 10 (existing rating is out of 10):\n\n\n\n\n\n\n\n\nSeries_Title\nRelease\nGenre\nIMDB_Rating\nDirector\n\n\n\n\nThe Shawshank Redemption\n1994\nDrama\n93\nFrank Darabont\n\n\nThe Godfather\n1972\nCrime\n92\nFrancis Ford Coppola\n\n\nThe Dark Knight\n2008\nAction\n90\nChristopher Nolan\n\n\nThe Godfather: Part II\n1974\nCrime\n90\nFrancis Ford Coppola\n\n\n12 Angry Men\n1957\nCrime\n90\nSidney Lumet\n\n\nThe Lord of the Rings: The Return of the King\n2003\nAction\n89\nPeter Jackson\n\n\nPulp Fiction\n1994\nCrime\n89\nQuentin Tarantino\n\n\nSchindler's List\n1993\nBiography\n89\nSteven Spielberg\n\n\nInception\n2010\nAction\n88\nChristopher Nolan\n\n\nFight Club\n1999\nDrama\n88\nDavid Fincher"
  },
  {
    "objectID": "posts/tidy-data/index.html#a-simple-example",
    "href": "posts/tidy-data/index.html#a-simple-example",
    "title": "What is tidy data?",
    "section": "A Simple Example",
    "text": "A Simple Example\nLet’s take an example of how we might intuitively look at a dataset to determine if its “tidy-ness”. Consider the following hypothetical table that is tracking student records at a local school.\n\n\n\n\n\n\n\n\nName\nStudentID\nAge_and_Sex\nScience_Score\nMath_Score\nEnglish_Score\n\n\n\n\nRobert\nS001\n18 M\n90\n95\n70\n\n\nJohn\nS002\n15 M\n88\n79\n60\n\n\nJane\nS003\n17 F\n50\n50\n55\n\n\nAbdul\nS004\n14 M\n70\n90\n100\n\n\nTracy\nS005\n16 F\n40\n100\n90\n\n\n\n\n\n\n\nWhat do you notice? Based off the rules that we have, we have a couple of violations. Let’s reflect this against the Tidy Principles:\n1. Each column is a variable.\nAlthough each column could be considered a variable, the context is wanting. Although Name and StudentID satisfy this rule, we can clearly see that the “Age_and_Sex” variables seems to represent two variables fused as one (Age and Sex).\nAlso notice the variables for each subject score - Science_Score, Math_Score and English_Score. These variable names seem not to be variables but rather values masked as variables - Science, Math and English. In essence, these values should be put under a new variable with an appropriate variable name (e.g. Subject) and the values - the scores - put within a second variable with an appropriate variable name (e.g. Score).\n2. Each row is an observation.\nAlthough each row in essence is an observation, the placement of the values would need to change given the above rule violations. Implementing the above changes would reduce the “length” of each observation - the number of variables captured per observation would reduce from 6 per observation to just 4 (Name, StudentID, Subject, and Score).\n3. Each cell (intersection between the column and row) is a SINGLE value.\nIt was evident that the “Age_And_Sex” column had two values instead of just a single value.\nGiven the above principle violations, the “untidy” data would probably be best structured as follows in order satisfying each of the three Tidy Principles:\n\n\n\n\n\n\n\n\nName\nStudentID\nAge\nSex\nSubject\nScore\n\n\n\n\nRobert\nS001\n18\nM\nScience\n90\n\n\nRobert\nS001\n18\nM\nMath\n95\n\n\nRobert\nS001\n18\nM\nEnglish\n70\n\n\nJohn\nS002\n15\nM\nScience\n88\n\n\nJohn\nS002\n15\nM\nMath\n79\n\n\nJohn\nS002\n15\nM\nEnglish\n60\n\n\nJane\nS003\n17\nF\nScience\n50\n\n\nJane\nS003\n17\nF\nMath\n50\n\n\nJane\nS003\n17\nF\nEnglish\n55\n\n\nAbdul\nS004\n14\nM\nScience\n70\n\n\nAbdul\nS004\n14\nM\nMath\n90\n\n\nAbdul\nS004\n14\nM\nEnglish\n100\n\n\nTracy\nS005\n16\nF\nScience\n40\n\n\nTracy\nS005\n16\nF\nMath\n100\n\n\nTracy\nS005\n16\nF\nEnglish\n90"
  },
  {
    "objectID": "posts/tidy-data/index.html#conclusion",
    "href": "posts/tidy-data/index.html#conclusion",
    "title": "What is tidy data?",
    "section": "Conclusion",
    "text": "Conclusion\nFocusing on cleaning one’s data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one’s analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
  },
  {
    "objectID": "posts/okr-support-innovation/index.html",
    "href": "posts/okr-support-innovation/index.html",
    "title": "How can OKRs Support Innovation Creation and Discovery?",
    "section": "",
    "text": "A lot has been talked about when it comes to Objectives and Key Results (OKRs). Some see it as the “holy grail” to improving organisational performance, whilst others see it as “just another performance management tool”. But, as a mental model, it works well in driving daily performance expectations, projects, and innovation discovery and creation.\nSo, how can OKRs support innovation? Let’s first understand briefly what OKRs are all about.\nOKRs can be seen as a performance management tool, but one that focuses on goals as a means to help guide an organisation, team or individual align towards delivering measurable and impactful outcomes. As one would notice, the words in bold are those typically used as pillars of innovative teams and activities.\n\nObjectives state a concrete, action-oriented and inspirational desired state, whilst Key Results help define specific, time bound, aggressive but realistic, measurable and verifiable targets.\n\nLet’s put it into context.\nAs a typical example, consider a company that is embarking on making its way of doing business sustainable and environmentally friendly. As a consultancy company, it has recognised that its carbon footprint is quite high compared to other companies in its industry.\nIt therefore creates a set of OKRs that can help the organisation start its journey towards this goal.\n\n\n\n\n\n\n\nObjective\nCreate the lowest carbon footprint in the consultancy industry\n\n\n\n\nKey Result 1\nReduce the use of paper printing in its report development process by 90%.\n\n\nKey Result 2\nReduce the company’s energy use by 50%.\n\n\nKey Result 3\nUse 100% reusable or recyclable material for all client events.\n\n\n\nAs we can see, the Objective is concrete, action-oriented and inspirational, whilst its accompanying Key Results are specific and aggressive, but are measurable and verifiable. We can assume for this example that these are for the entire year (time bound).\nHow about innovation?\nNot all organisations are the same when it comes to managing innovation. In my opinion, innovation should be part of how organisations do work as opposed to a “plug-in” where it comes in after the fact. Innovation should be embedded implicitly in the way organisations set out their strategy and goals, but rewarded explicitly to help drive the creative culture an organisation strives to have. A culture of innovation expedites the speed of an organisation in meeting its strategic goals and allows for new challenges to quickly come on-board.\nAs an example, let’s assume a Fintech is looking at adding Investments as a product line and sees building a savings culture with its customers as a potential strategy to increase its liquidity for investing. It therefore has “making savings a way of life” as a company-wide objective and identifies 3 key results that will define its success:\n\n\n\n\n\n\n\nObjective\nMake savings a way of life for its customers\n\n\n\n\nKey Result 1\nMake its saving product the best performing product within 2 years in its portfolio.\n\n\nKey Result 2\nMake enrolment and use of the product as seamless as possible.\n\n\nKey Result 3\nAt least 40% of its customer base should be enrolled onto the product by the end of the financial year.\n\n\n\nThe company’s product and technology teams for this company decide to pick up on this new objective and create new team objectives that feed into Key Results 1 and 2 respectively.\nThe product team may leverage on its design thinking approach to craft a set of key results that can enable the team identify a potentially new product for the company:\n\n\n\n\n\n\n\nObjective\nCreate a saving product that is the best offering in the company’s portfolio\n\n\n\n\nKey Result 1\nTalk to 80 new and 50 existing customers by the end of January.\n\n\nKey Result 2\nPropose (at most) 3 pain points that could be solved by savings by the end of January.\n\n\nKey Result 3\nTest at least one high-fidelity potential prototype with at least 80 customers by the end of March.\n\n\nKey Result 4\nPresent a viable product for scaling by 15 April.\n\n\n\nThe technology team for this company may craft OKRs aligned to the Key Result 2 as follows:\n\n\n\n\n\n\n\nObjective\nBuild technology that enables a seamless customer saving experience by end of Q4\n\n\n\n\nKey Result 1\nLaunch mobile app by by end of Q3.\n\n\nKey Result 2\nEnable real-time reporting on product adoption and usage by Q3.\n\n\nKey Result 3\nScale infrastructure to support 1,000,000 customers by the end of Q4.\n\n\nKey Result 4\nEnsure 100% compliance to GDPR and related data protection laws.\n\n\n\nThese Key Results may be further broken down by teams within the technology department that may be in charge of software development, infrastructure support, information security and others.\nThis is where the alignment of OKRs come into fruition - a pyramid of objectives and key results can be created across teams, each feeding into the other to ensure alignment to the overall strategic goal.\nOKRs define success and set “what” needs to be accomplished, not “how” it should be accomplished. This allows for teams to create initiatives that can meet the aggressive key results, the beauty being that teams will need to think differently and out-of-the-box to create really innovative solutions that can rise to these hard targets.\nIf implemented well, OKRs could be the answer to many organisations looking to embed innovation as part of its culture. The only thing that remains is creating an ecosystem of knowledge and partners, and a recognition and rewards system that can empower, inspire and allow staff to try, fail, learn and innovate to their best!"
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html",
    "href": "posts/exploratory-analysis-social-media/index.html",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "",
    "text": "I was asked to teach a class on how to import, clean and transform data in R and as part of the training. We did several exercises to help re-enforce the basics and, given that it was well received, I decided to document these exercises in a series that I’m calling the Data Cleaning Series.\nIt will also serve as a journal for me to refer to when (being human) I forget 😅.\nIf you haven’t done so yet I recommend you read my brief entry on tidy data as we shall endeavour to put our data in this format throughout the series."
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html#background-to-the-data-cleaning-series",
    "href": "posts/exploratory-analysis-social-media/index.html#background-to-the-data-cleaning-series",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "",
    "text": "I was asked to teach a class on how to import, clean and transform data in R and as part of the training. We did several exercises to help re-enforce the basics and, given that it was well received, I decided to document these exercises in a series that I’m calling the Data Cleaning Series.\nIt will also serve as a journal for me to refer to when (being human) I forget 😅.\nIf you haven’t done so yet I recommend you read my brief entry on tidy data as we shall endeavour to put our data in this format throughout the series."
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html#about-the-dataset---social-media-addiction-analysis",
    "href": "posts/exploratory-analysis-social-media/index.html#about-the-dataset---social-media-addiction-analysis",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "About the dataset - Social Media Addiction Analysis",
    "text": "About the dataset - Social Media Addiction Analysis\nFor this exercise, we are leveraging on the Social Media Addiction dataset provided by Anil Shamim from the Kaggle Platform.\nThe data set is a set of anonymized records of students’ social media behaviors and related life outcomes, spanning multiple countries and background. Each row represents one student’s survey response, offering a cross‐sectional snapshot suitable for statistical analysis and machine‐learning applications.\nFor full details of the dataset, I would encourage you to visit the site and view its Data Card.\nMake sure you download the data set and save it on your machine (you might need a free Kaggle account to get it).\nLet’s import it in and do a bit of exploration to get to know it better:\n\nsocial_media_data &lt;- read_csv('Students_Social_Media_Addiction.csv')\n\nRows: 705 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Gender, Academic_Level, Country, Most_Used_Platform, Affects_Academ...\ndbl (7): Student_ID, Age, Avg_Daily_Usage_Hours, Sleep_Hours_Per_Night, Ment...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsocial_media_data\n\n# A tibble: 705 × 13\n   Student_ID   Age Gender Academic_Level Country     Avg_Daily_Usage_Hours\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;                       &lt;dbl&gt;\n 1          1    19 Female Undergraduate  Bangladesh                    5.2\n 2          2    22 Male   Graduate       India                         2.1\n 3          3    20 Female Undergraduate  USA                           6  \n 4          4    18 Male   High School    UK                            3  \n 5          5    21 Male   Graduate       Canada                        4.5\n 6          6    19 Female Undergraduate  Australia                     7.2\n 7          7    23 Male   Graduate       Germany                       1.5\n 8          8    20 Female Undergraduate  Brazil                        5.8\n 9          9    18 Male   High School    Japan                         4  \n10         10    21 Female Graduate       South Korea                   3.3\n# ℹ 695 more rows\n# ℹ 7 more variables: Most_Used_Platform &lt;chr&gt;,\n#   Affects_Academic_Performance &lt;chr&gt;, Sleep_Hours_Per_Night &lt;dbl&gt;,\n#   Mental_Health_Score &lt;dbl&gt;, Relationship_Status &lt;chr&gt;,\n#   Conflicts_Over_Social_Media &lt;dbl&gt;, Addicted_Score &lt;dbl&gt;\n\n\nFrom the output, we notice that the dataset has 705 variables and 13 observations. Given that we have the Data Caard that outlines the expected values for each variable, we can validate the information to make sure that it conforms to what was expected:\n\n# Let's build the validation engine based on the rules on the Data Card.\nvalidation_rules &lt;- validator(\n  is_unique(Student_ID), \n  Age &gt; 0, \n  Gender %in% c(\"Male\", \"Female\"), \n  Academic_Level %in% c(\"High School\", \"Undergraduate\", \"Graduate\"), \n  in_range(Avg_Daily_Usage_Hours, 0, 24),\n  Affects_Academic_Performance %in% c(\"Yes\", \"No\"),\n  Sleep_Hours_Per_Night &gt; 0,\n  in_range(Mental_Health_Score, 1, 10),\n  Relationship_Status %in% c(\"Single\", \"In Relationship\", \"Complicated\"),\n  Conflicts_Over_Social_Media &gt;= 0,\n  in_range(Addicted_Score, 1, 10),\n  is.character(Country), \n  is.character(Most_Used_Platform)\n)\n\n# Let's validate and assess the output.\nconfront(social_media_data, validation_rules)\n\nObject of class 'validation'\nCall:\n    confront(dat = social_media_data, x = validation_rules)\n\nRules confronted: 13\n   With fails   : 0\n   With missings: 0\n   Threw warning: 0\n   Threw error  : 0\n\n\nFrom the output, it is clear that all the variables satisfy the set constraints. However, as a sanity check for Country and Most_Used_Platform, let’s print out their unique values to see whether they are a good representation of what currently exists:\n\n# Let's just view the unique value of the Country variable\n\nsocial_media_data |&gt; \n  select(Country) |&gt; \n  unique() |&gt; \n  pull()\n\n  [1] \"Bangladesh\"      \"India\"           \"USA\"             \"UK\"             \n  [5] \"Canada\"          \"Australia\"       \"Germany\"         \"Brazil\"         \n  [9] \"Japan\"           \"South Korea\"     \"France\"          \"Spain\"          \n [13] \"Italy\"           \"Mexico\"          \"Russia\"          \"China\"          \n [17] \"Sweden\"          \"Norway\"          \"Denmark\"         \"Netherlands\"    \n [21] \"Belgium\"         \"Switzerland\"     \"Austria\"         \"Portugal\"       \n [25] \"Greece\"          \"Ireland\"         \"New Zealand\"     \"Singapore\"      \n [29] \"Malaysia\"        \"Thailand\"        \"Vietnam\"         \"Philippines\"    \n [33] \"Indonesia\"       \"Taiwan\"          \"Hong Kong\"       \"Turkey\"         \n [37] \"Israel\"          \"UAE\"             \"Egypt\"           \"Morocco\"        \n [41] \"South Africa\"    \"Nigeria\"         \"Kenya\"           \"Ghana\"          \n [45] \"Argentina\"       \"Chile\"           \"Colombia\"        \"Peru\"           \n [49] \"Venezuela\"       \"Ecuador\"         \"Uruguay\"         \"Paraguay\"       \n [53] \"Bolivia\"         \"Costa Rica\"      \"Panama\"          \"Jamaica\"        \n [57] \"Trinidad\"        \"Bahamas\"         \"Iceland\"         \"Finland\"        \n [61] \"Poland\"          \"Romania\"         \"Hungary\"         \"Czech Republic\" \n [65] \"Slovakia\"        \"Croatia\"         \"Serbia\"          \"Slovenia\"       \n [69] \"Bulgaria\"        \"Estonia\"         \"Latvia\"          \"Lithuania\"      \n [73] \"Ukraine\"         \"Moldova\"         \"Belarus\"         \"Kazakhstan\"     \n [77] \"Uzbekistan\"      \"Kyrgyzstan\"      \"Tajikistan\"      \"Armenia\"        \n [81] \"Georgia\"         \"Azerbaijan\"      \"Cyprus\"          \"Malta\"          \n [85] \"Luxembourg\"      \"Monaco\"          \"Andorra\"         \"San Marino\"     \n [89] \"Vatican City\"    \"Liechtenstein\"   \"Montenegro\"      \"Albania\"        \n [93] \"North Macedonia\" \"Kosovo\"          \"Bosnia\"          \"Qatar\"          \n [97] \"Kuwait\"          \"Bahrain\"         \"Oman\"            \"Jordan\"         \n[101] \"Lebanon\"         \"Iraq\"            \"Yemen\"           \"Syria\"          \n[105] \"Afghanistan\"     \"Pakistan\"        \"Nepal\"           \"Bhutan\"         \n[109] \"Sri Lanka\"       \"Maldives\"       \n\n\n\n# Let's just view the unique values of Most_Used_Platform variable\n\nsocial_media_data |&gt; \n  select(Most_Used_Platform) |&gt; \n  unique() |&gt; \n  pull()\n\n [1] \"Instagram\" \"Twitter\"   \"TikTok\"    \"YouTube\"   \"Facebook\"  \"LinkedIn\" \n [7] \"Snapchat\"  \"LINE\"      \"KakaoTalk\" \"VKontakte\" \"WhatsApp\"  \"WeChat\"   \n\n\nWe can see that both outputs seem reasonable.\nFrom the data, we can also infer that Academic_Level is an ordered factor variable. Although not necessary for this exercise, it aids the presentation to have results using this variable in a chronological order. Let’s fix this:\n\nsocial_media_data &lt;- social_media_data |&gt; \n  mutate(\n    Academic_Level = fct(Academic_Level, \n                         levels = c(\"High School\", \"Undergraduate\", \"Graduate\"))\n  )\n  \nglimpse(social_media_data)\n\nRows: 705\nColumns: 13\n$ Student_ID                   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n$ Age                          &lt;dbl&gt; 19, 22, 20, 18, 21, 19, 23, 20, 18, 21, 1…\n$ Gender                       &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Male\", \"Male…\n$ Academic_Level               &lt;fct&gt; Undergraduate, Graduate, Undergraduate, H…\n$ Country                      &lt;chr&gt; \"Bangladesh\", \"India\", \"USA\", \"UK\", \"Cana…\n$ Avg_Daily_Usage_Hours        &lt;dbl&gt; 5.2, 2.1, 6.0, 3.0, 4.5, 7.2, 1.5, 5.8, 4…\n$ Most_Used_Platform           &lt;chr&gt; \"Instagram\", \"Twitter\", \"TikTok\", \"YouTub…\n$ Affects_Academic_Performance &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ Sleep_Hours_Per_Night        &lt;dbl&gt; 6.5, 7.5, 5.0, 7.0, 6.0, 4.5, 8.0, 6.0, 6…\n$ Mental_Health_Score          &lt;dbl&gt; 6, 8, 5, 7, 6, 4, 9, 6, 7, 7, 5, 6, 8, 5,…\n$ Relationship_Status          &lt;chr&gt; \"In Relationship\", \"Single\", \"Complicated…\n$ Conflicts_Over_Social_Media  &lt;dbl&gt; 3, 0, 4, 1, 2, 5, 0, 2, 1, 1, 3, 2, 1, 4,…\n$ Addicted_Score               &lt;dbl&gt; 8, 3, 9, 4, 7, 9, 2, 8, 5, 4, 7, 8, 4, 9,…\n\n\nAdditional data cleaning may be needed but we shall further clean as part of answering the questions.\nLet’s proceed to answering the questions!\n\nQuestion 1\n\nWhat is the total number of male and female students by country?\n\nsocial_media_data |&gt; \n  summarise(total = n(), .by = Gender) |&gt; \n  mutate(\n    perc_gender = round(\n      total / sum(total) * 100, 2))\n\n# A tibble: 2 × 3\n  Gender total perc_gender\n  &lt;chr&gt;  &lt;int&gt;       &lt;dbl&gt;\n1 Female   353        50.1\n2 Male     352        49.9\n\n\nWe can see that its an almost equal split between the Female and the Male gender - 50.07% against 49.93% respectively.\n\n\n\nQuestion 2\n\nWhich social media platform is associated with affecting the largest number of students by Academic Level?\n\nsocial_media_data |&gt; \n  group_by(Academic_Level, Most_Used_Platform) |&gt; \n  summarise(\n    count = n()) |&gt; \n  arrange(\n    desc(count)) |&gt; # Data is still grouped allowing us to order each group by the count\n  slice_head(n = 1) |&gt; \n  ungroup() # Remove groupings. Although not necessary at this point, its a good habit.\n\n`summarise()` has grouped output by 'Academic_Level'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 3 × 3\n  Academic_Level Most_Used_Platform count\n  &lt;fct&gt;          &lt;chr&gt;              &lt;int&gt;\n1 High School    Instagram             12\n2 Undergraduate  Instagram            155\n3 Graduate       Facebook              87\n\n\nGiven that we ordered the factors for Academic Level, we can see that its chronologically ordered in the output. We notice that two social media platforms stand out as popular - Instagram for High School and Undergraduate, and Facebook for Graduate.\n\n\n\nQuestion 3\n\nShow a table of countries showing the average sleep hours per night of High School students, with countries ordered from the least amount of sleep to the most.\n\nsocial_media_data |&gt; \n  filter(Academic_Level == \"High School\") |&gt; \n  select(Country, Sleep_Hours_Per_Night) |&gt; \n  summarise(Avg_Sleep_Hours = mean(Sleep_Hours_Per_Night), \n            .by = Country) |&gt; \n  arrange(Avg_Sleep_Hours)\n\n# A tibble: 27 × 2\n   Country        Avg_Sleep_Hours\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 UAE                        5.1\n 2 Ireland                    5.2\n 3 Ecuador                    5.2\n 4 Trinidad                   5.2\n 5 Czech Republic             5.2\n 6 Armenia                    5.2\n 7 Liechtenstein              5.2\n 8 Lebanon                    5.2\n 9 Thailand                   5.3\n10 Argentina                  5.3\n# ℹ 17 more rows\n\n\nFrom the results, we can see that the UAE has the lowest average sleep per night at only 5.1 hours.\n\n\n\nQuestion 4\n\nFind the average addiction score for each country. Include the country’s respective continent and order the table in descending order by the country’s average addiction score\nThis is an interesting problem. Given that the dataset requires continents, we will need another dataset having a listing of countries in their respective continent, and then perform a join in order to allow us find the average addiction score per country with their respective continent.\nRather than manually creating the file, we will scour the internet to find a dataset that is relatively up-to-date and reliable and import it into our environment.\n\n# Let's first download the continent dataset and extract the columns of interest\n# (we will also extract the Capital column as this is needed for Question 6)\n\ncountries_continents &lt;- \n  read_delim(\n    file = 'https://www.countrycode.org/countryCode/downloadCountryCodes', \n    col_select = c(\"Country Name\", \"Continent\", \"Capital\"), \n    delim = ',') |&gt;\n  janitor::clean_names()\n\nRows: 240 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Country Name, Continent, Capital\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Let's see whether there are any countries from our social media dataset that is missing from our continent dataset\n\nsocial_media_data |&gt; \n  anti_join(\n    countries_continents, \n    join_by(Country == country_name)) |&gt; \n  select(Country) |&gt; \n  unique()\n\n# A tibble: 7 × 1\n  Country        \n  &lt;chr&gt;          \n1 USA            \n2 UK             \n3 UAE            \n4 Trinidad       \n5 Vatican City   \n6 North Macedonia\n7 Bosnia         \n\n\nForm the output, we can see that there are 7 countries that exist in our social media dataset that do not match our continent dataset. In principle, these countries do exist but possibly in a different structure.\nLet’s see whether we can find them.\n\ncountries_continents |&gt;\n  select(country_name) |&gt; \n  mutate(\n    found = str_detect(\n      string = country_name, \n      pattern = \"United|Trinidad|Vatican|Macedonia|Bosnia\")\n  ) |&gt; \n  filter(found)\n\n# A tibble: 7 × 2\n  country_name           found\n  &lt;chr&gt;                  &lt;lgl&gt;\n1 Bosnia and Herzegovina TRUE \n2 Macedonia              TRUE \n3 Trinidad and Tobago    TRUE \n4 United Arab Emirates   TRUE \n5 United Kingdom         TRUE \n6 United States          TRUE \n7 Vatican                TRUE \n\n\nThis proved our hypothesis that the countries did indeed exist in our continent dataset but under a different name. Now that we know what names they are written in, we can clean our social media dataset to match the appropriate names so that our joins work properly.\n\n# Let's manually clean up those countries in our social media dataset\n# so that they match our continent dataset. \n\nsocial_media_data &lt;- social_media_data |&gt;\n  mutate(\n    Country = case_when(\n      str_detect(Country, \"USA\") ~ \"United States\",\n      str_detect(Country, \"UK\") ~ \"United Kingdom\", \n      str_detect(Country, \"UAE\") ~ \"United Arab Emirates\", \n      str_detect(Country, \"Vatican City\") ~ \"Vatican\",\n      str_detect(Country, \"North Macedonia\") ~ \"Macedonia\", \n      str_detect(Country, \"Trinidad\") ~ \"Trinidad and Tobago\", \n      str_detect(Country, \"Bosnia\") ~ \"Bosnia and Herzegovina\", \n      TRUE ~ Country\n    ))\n\n# Let's see whether the countries have been replaced\n\nsocial_media_data |&gt; \n  select(Country) |&gt; \n  arrange(Country) |&gt;\n  unique() |&gt; \n  pull()\n\n  [1] \"Afghanistan\"            \"Albania\"                \"Andorra\"               \n  [4] \"Argentina\"              \"Armenia\"                \"Australia\"             \n  [7] \"Austria\"                \"Azerbaijan\"             \"Bahamas\"               \n [10] \"Bahrain\"                \"Bangladesh\"             \"Belarus\"               \n [13] \"Belgium\"                \"Bhutan\"                 \"Bolivia\"               \n [16] \"Bosnia and Herzegovina\" \"Brazil\"                 \"Bulgaria\"              \n [19] \"Canada\"                 \"Chile\"                  \"China\"                 \n [22] \"Colombia\"               \"Costa Rica\"             \"Croatia\"               \n [25] \"Cyprus\"                 \"Czech Republic\"         \"Denmark\"               \n [28] \"Ecuador\"                \"Egypt\"                  \"Estonia\"               \n [31] \"Finland\"                \"France\"                 \"Georgia\"               \n [34] \"Germany\"                \"Ghana\"                  \"Greece\"                \n [37] \"Hong Kong\"              \"Hungary\"                \"Iceland\"               \n [40] \"India\"                  \"Indonesia\"              \"Iraq\"                  \n [43] \"Ireland\"                \"Israel\"                 \"Italy\"                 \n [46] \"Jamaica\"                \"Japan\"                  \"Jordan\"                \n [49] \"Kazakhstan\"             \"Kenya\"                  \"Kosovo\"                \n [52] \"Kuwait\"                 \"Kyrgyzstan\"             \"Latvia\"                \n [55] \"Lebanon\"                \"Liechtenstein\"          \"Lithuania\"             \n [58] \"Luxembourg\"             \"Macedonia\"              \"Malaysia\"              \n [61] \"Maldives\"               \"Malta\"                  \"Mexico\"                \n [64] \"Moldova\"                \"Monaco\"                 \"Montenegro\"            \n [67] \"Morocco\"                \"Nepal\"                  \"Netherlands\"           \n [70] \"New Zealand\"            \"Nigeria\"                \"Norway\"                \n [73] \"Oman\"                   \"Pakistan\"               \"Panama\"                \n [76] \"Paraguay\"               \"Peru\"                   \"Philippines\"           \n [79] \"Poland\"                 \"Portugal\"               \"Qatar\"                 \n [82] \"Romania\"                \"Russia\"                 \"San Marino\"            \n [85] \"Serbia\"                 \"Singapore\"              \"Slovakia\"              \n [88] \"Slovenia\"               \"South Africa\"           \"South Korea\"           \n [91] \"Spain\"                  \"Sri Lanka\"              \"Sweden\"                \n [94] \"Switzerland\"            \"Syria\"                  \"Taiwan\"                \n [97] \"Tajikistan\"             \"Thailand\"               \"Trinidad and Tobago\"   \n[100] \"Turkey\"                 \"Ukraine\"                \"United Arab Emirates\"  \n[103] \"United Kingdom\"         \"United States\"          \"Uruguay\"               \n[106] \"Uzbekistan\"             \"Vatican\"                \"Venezuela\"             \n[109] \"Vietnam\"                \"Yemen\"                 \n\n\nFrom the list, we can see that they have been appropriately replaced.\nLet’s now do our join and effectively answer the question.\n\nsocial_media_data |&gt; \n  left_join(countries_continents, join_by(Country == country_name)) |&gt;\n  select(continent, Country, Addicted_Score) |&gt; \n  summarise(\n    avg_addicted_score = round(mean(Addicted_Score), 2), \n    .by = c(continent, Country)) |&gt; \n  arrange(desc(avg_addicted_score))\n\n# A tibble: 110 × 3\n   continent     Country              avg_addicted_score\n   &lt;chr&gt;         &lt;chr&gt;                             &lt;dbl&gt;\n 1 South America Ecuador                            9   \n 2 Europe        Czech Republic                     9   \n 3 Asia          Armenia                            9   \n 4 Europe        Liechtenstein                      9   \n 5 Asia          Lebanon                            9   \n 6 North America United States                      8.6 \n 7 Asia          United Arab Emirates               8.12\n 8 Europe        Norway                             8   \n 9 Europe        Belgium                            8   \n10 Europe        Portugal                           8   \n# ℹ 100 more rows\n\n\nFrom the list, we can see that the countries with the highest average addiction score (9.0) include Ecuador (South America), Czech Republic (Europe), Armenia (Asia) and Liechtenstein (Europe).\n\n\n\nQuestion 5\n\nWhich gender per country and continent is the most lonely? Assume “being lonely” are those in a relationship status of “Single” and “Complicated”.\n\nsocial_media_data |&gt; \n  select(\n    Country, Gender, Relationship_Status) |&gt; \n  filter(\n    # Only focusing on Single and Complicated Relationships\n    Relationship_Status %in% c(\"Single\", \"Complicated\")) |&gt;\n  summarise(\n    # Count the number of respondents by Country and Gender\n    total = n(),\n    .by = c(\"Country\", \"Gender\")) |&gt; \n  mutate(\n    # Not necessary but its good to see what % of men and women per country are within \n    # this relationship status grouping\n    perc_total = total / sum(total) * 100, \n    .by = Country\n  ) |&gt; \n  filter(\n    # Filter the gender having the largest number of singles and complicated relationships\n    # in each Country.\n    total == max(total), \n    .by = Country) |&gt; \n  left_join(\n    # And let's get the continent information into the final result.\n    countries_continents, \n    join_by(\n      Country == country_name)) |&gt; \n  select(\n    # Select what we need to see\n    continent, Country, Gender, total, perc_total) |&gt; \n  arrange(desc(total)) # And order the results by number of individuals\n\n# A tibble: 85 × 5\n   continent     Country        Gender total perc_total\n   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1 North America Canada         Male      28       90.3\n 2 Europe        Ireland        Female    23       85.2\n 3 Europe        Spain          Female    22       84.6\n 4 Asia          Turkey         Male      22       81.5\n 5 North America Mexico         Male      22       81.5\n 6 Europe        Switzerland    Female    18      100  \n 7 Asia          Japan          Female    16       76.2\n 8 Europe        Poland         Male      12       75  \n 9 Europe        United Kingdom Male      11       73.3\n10 Asia          Bangladesh     Female    10       55.6\n# ℹ 75 more rows\n\n\nFrom the data, we can see that Canadian men are more lonely than their women counterparts, with approximately 9 in 10 identifying as Single or in a Complicated relationship.\nThis is followed by Ireland and Spain in 2nd and 3rd place with Women identifying as Single or in a Complicated relationship (consituting approximately 85% of the respondents from each of those countries).\n\n\n\nQuestion 6\n\nWhich capital city (and country) has the most number of students “In Relationship”?\nWe can leverage the same thought process as in Question 5, but tweak it slightly.\nPS: For the enlightened, this would probably constitute the need for a function to avoid duplicated code but we shall cross that bridge in another write-up!\n\nsocial_media_data |&gt; \n  select(\n    Country, Gender, Relationship_Status) |&gt; \n  filter(\n    # THE FIRST TWEAK! Changing the relationship type\n    Relationship_Status %in% c(\"In Relationship\")) |&gt;\n  summarise(\n    # Count the number of respondents by Country and Gender\n    total = n(),\n    .by = c(\"Country\", \"Gender\")) |&gt; \n  mutate(\n    # Not necessary but its good to see what % of men and women per country are within \n    # this relationship status grouping\n    perc_total = total / sum(total) * 100, \n    .by = Country\n  ) |&gt; \n  filter(\n    # Filter the gender having the largest number of singles and complicated relationships\n    # in each Country.\n    total == max(total), \n    .by = Country) |&gt; \n  left_join(\n    # SECOND TWEAK - Let's get the CAPITAL CITY information into the final result.\n    countries_continents, \n    join_by(\n      Country == country_name)) |&gt; \n  select(\n    # Select what we need to see\n    Country, capital, Gender, total, perc_total) |&gt; \n  arrange(desc(total)) # And order the results by number of individuals\n\n# A tibble: 49 × 5\n   Country              capital    Gender total perc_total\n   &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1 India                New Delhi  Male      28       65.1\n 2 United States        Washington Female    28       87.5\n 3 Denmark              Copenhagen Male      22       84.6\n 4 France               Paris      Female    21       87.5\n 5 China                Beijing    Male      11       73.3\n 6 Pakistan             Islamabad  Male      10       55.6\n 7 Maldives             Male       Male      10       58.8\n 8 Italy                Rome       Male      10       71.4\n 9 United Arab Emirates Abu Dhabi  Female     8      100  \n10 South Korea          Seoul      Female     7      100  \n# ℹ 39 more rows\n\n\nFrom the data, we can see that India and the United States - with its’ capital cities as New Delhi and Washington respectively - is leading the pack, with 28 of the males and females identifying as in a relationship. This constitutes to approximately 65% and 88% of the total respondents by respective country.\n\n\n\nQuestion 7\n\nDetermine the most popular social media platform in each country and continent\n\npopular_social_media_results &lt;- social_media_data |&gt; \n  select(\n    Country, Most_Used_Platform) |&gt; \n  summarise(\n    # Count the number of social media platform selected by respondents \n    # by Country\n    total = n(),\n    .by = c(Country, Most_Used_Platform)) |&gt; \n  filter(\n    # Filter the most popular social media platform for each country\n    total == max(total), \n    .by = Country) |&gt; \n  left_join(\n    # Let's get the continent information into the final result.\n    countries_continents, \n    join_by(\n      Country == country_name)) |&gt; \n  select(\n    # Select what we need to see\n    continent, Country, Most_Used_Platform, total) |&gt; \n  arrange(desc(total)) # And order the results by number of individuals\n\npopular_social_media_results\n\n# A tibble: 110 × 4\n   continent     Country       Most_Used_Platform total\n   &lt;chr&gt;         &lt;chr&gt;         &lt;chr&gt;              &lt;int&gt;\n 1 Asia          India         WhatsApp              25\n 2 Europe        France        Instagram             22\n 3 Europe        Switzerland   Instagram             20\n 4 Europe        Denmark       Facebook              19\n 5 North America United States Instagram             19\n 6 Asia          Pakistan      Instagram             18\n 7 North America Mexico        WhatsApp              18\n 8 North America Canada        Instagram             17\n 9 Europe        Ireland       Instagram             16\n10 Asia          China         WeChat                15\n# ℹ 100 more rows\n\n\nFrom this, we can see the most popular social media platform by country. India lists as having WhatsApp as the most popular from the respondents, with France and Italy reporting Instagram as their most popular.\nWe can further aggregate this result by continent to see which platform is the most popular.\n\npopular_social_media_results |&gt; \n  summarise(\n    total = sum(total), \n    .by = c(continent, Most_Used_Platform)\n  ) |&gt; \n  filter(\n    total == max(total), \n    .by = continent)\n\n# A tibble: 11 × 3\n   continent     Most_Used_Platform total\n   &lt;chr&gt;         &lt;chr&gt;              &lt;int&gt;\n 1 Europe        Instagram             80\n 2 North America Instagram             37\n 3 Asia          Instagram             57\n 4 Oceania       Instagram             12\n 5 South America WhatsApp               4\n 6 Africa        YouTube                1\n 7 Africa        TikTok                 1\n 8 Africa        LinkedIn               1\n 9 Africa        Instagram              1\n10 Africa        Facebook               1\n11 Africa        Snapchat               1\n\n\nIn Europe, Instagram has a clear lead with over 80 European respondents claiming the social media platform is their most used. North America, Asia and Oceania report the same platform while South America has WhatsApp. Interestingly, Africa seems to have no preference, with almost all being just as popular as the other."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Data Cleaning Series :: Exploring Social Media Addiction Data\n\n\n\n\n\n\ndata-cleaning\n\n\ndata-exploration\n\n\n\n\n\n\n\n\n\nJul 22, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday - 2021 Week 29 [Scooby Doo]\n\n\n\n\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday - 2022 Week 41 [Ravelry Yarn]\n\n\n\n\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nRegex-ing in R\n\n\n\n\n\n\ndata-cleaning\n\n\n\n\n\n\n\n\n\nMay 30, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is tidy data?\n\n\n\n\n\n\ndata-cleaning\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nHow can OKRs Support Innovation Creation and Discovery?\n\n\n\n\n\n\nperformance\n\n\ninnovation\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tt-2021-29/index.html",
    "href": "posts/tt-2021-29/index.html",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "",
    "text": "This week’s tidytuesday dataset is provided by plummye who manually aggregated the Scooby Doo episode data, watching every Scooby-Doo iteration and tracking every variable."
  },
  {
    "objectID": "posts/tt-2021-29/index.html#scooby-doo-episodes",
    "href": "posts/tt-2021-29/index.html#scooby-doo-episodes",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "",
    "text": "This week’s tidytuesday dataset is provided by plummye who manually aggregated the Scooby Doo episode data, watching every Scooby-Doo iteration and tracking every variable."
  },
  {
    "objectID": "posts/tt-2021-29/index.html#about-the-data",
    "href": "posts/tt-2021-29/index.html#about-the-data",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "About the data",
    "text": "About the data\nThe has 603 observations across 75 variables, running from 1969-09-13 to 2021-02-25.\n\nscooby &lt;- tuesdata$scoobydoo\nscooby\n\n# A tibble: 603 × 75\n   index series_name   network season title imdb  engagement date_aired run_time\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;date&gt;        &lt;dbl&gt;\n 1     1 Scooby Doo, … CBS     1      What… 8.1   556        1969-09-13       21\n 2     2 Scooby Doo, … CBS     1      A Cl… 8.1   479        1969-09-20       22\n 3     3 Scooby Doo, … CBS     1      Hass… 8     455        1969-09-27       21\n 4     4 Scooby Doo, … CBS     1      Mine… 7.8   426        1969-10-04       21\n 5     5 Scooby Doo, … CBS     1      Deco… 7.5   391        1969-10-11       21\n 6     6 Scooby Doo, … CBS     1      What… 8.4   384        1969-10-18       21\n 7     7 Scooby Doo, … CBS     1      Neve… 7.6   358        1969-10-25       21\n 8     8 Scooby Doo, … CBS     1      Foul… 8.2   358        1969-11-01       21\n 9     9 Scooby Doo, … CBS     1      The … 8.1   371        1969-11-08       21\n10    10 Scooby Doo, … CBS     1      Bedl… 8     346        1969-11-15       21\n# ℹ 593 more rows\n# ℹ 66 more variables: format &lt;chr&gt;, monster_name &lt;chr&gt;, monster_gender &lt;chr&gt;,\n#   monster_type &lt;chr&gt;, monster_subtype &lt;chr&gt;, monster_species &lt;chr&gt;,\n#   monster_real &lt;chr&gt;, monster_amount &lt;dbl&gt;, caught_fred &lt;chr&gt;,\n#   caught_daphnie &lt;chr&gt;, caught_velma &lt;chr&gt;, caught_shaggy &lt;chr&gt;,\n#   caught_scooby &lt;chr&gt;, captured_fred &lt;chr&gt;, captured_daphnie &lt;chr&gt;,\n#   captured_velma &lt;chr&gt;, captured_shaggy &lt;chr&gt;, captured_scooby &lt;chr&gt;, …"
  },
  {
    "objectID": "posts/tt-2021-29/index.html#conclusion",
    "href": "posts/tt-2021-29/index.html#conclusion",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "Conclusion",
    "text": "Conclusion\nFocusing on cleaning one’s data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one’s analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog.\nOn this site, I shall be posting about different aspects of my work, life and hobbies that I feel will help others going through similar things on their own life journeys. Although it may be heavily biased towards my learnings on data and analytics, it will also serve as a bit of a journal helping me to put my thoughts on a ‚Äúpiece of paper‚Äù.\nDo enjoy the works and if you want to link up, hit me up on Bluesky!"
  },
  {
    "objectID": "posts/tidy-data/index.html",
    "href": "posts/tidy-data/index.html",
    "title": "What is tidy data?",
    "section": "",
    "text": "Tidy data is a philosophy often used in the data analysis circles to help structure data before using it for analysis. Although getting it into this form is a lot of hard work, it is akin to sharpening the axe before cutting down a tree - it may be a lot of work to get the tool (data) in the right state but it pays off immensely once the cutting (analysis) starts.\nIn the R community, putting your data in a tidy state allows one to leverage on the tidyverse packages that are created to operate on data complying with this philosophy. This philosophy, coupled with packages that comply with it, makes analysis such a joy.\nThis article is focused on helping you understand what it means to have data in a tidy state."
  },
  {
    "objectID": "posts/tidy-data/index.html#background",
    "href": "posts/tidy-data/index.html#background",
    "title": "What is tidy data?",
    "section": "",
    "text": "Tidy data is a philosophy often used in the data analysis circles to help structure data before using it for analysis. Although getting it into this form is a lot of hard work, it is akin to sharpening the axe before cutting down a tree - it may be a lot of work to get the tool (data) in the right state but it pays off immensely once the cutting (analysis) starts.\nIn the R community, putting your data in a tidy state allows one to leverage on the tidyverse packages that are created to operate on data complying with this philosophy. This philosophy, coupled with packages that comply with it, makes analysis such a joy.\nThis article is focused on helping you understand what it means to have data in a tidy state."
  },
  {
    "objectID": "posts/tidy-data/index.html#principles-behind-tidy-data",
    "href": "posts/tidy-data/index.html#principles-behind-tidy-data",
    "title": "What is tidy data?",
    "section": "Principles behind tidy data",
    "text": "Principles behind tidy data\nData can be represented in several ways. Consider a simple dataset that shows a table of Series with their release date, genre, IMDB rating and director.\nTable 1 might looks like the following:\n\n\n\n\n\n\n\n\nSeries_Title\nRelease\nGenre\nIMDB_Rating\nDirector\n\n\n\n\nThe Shawshank Redemption\n1994\nDrama\n9.3\nFrank Darabont\n\n\nThe Godfather\n1972\nCrime\n9.2\nFrancis Ford Coppola\n\n\nThe Dark Knight\n2008\nAction\n9.0\nChristopher Nolan\n\n\nThe Godfather: Part II\n1974\nCrime\n9.0\nFrancis Ford Coppola\n\n\n12 Angry Men\n1957\nCrime\n9.0\nSidney Lumet\n\n\nThe Lord of the Rings: The Return of the King\n2003\nAction\n8.9\nPeter Jackson\n\n\nPulp Fiction\n1994\nCrime\n8.9\nQuentin Tarantino\n\n\nSchindler's List\n1993\nBiography\n8.9\nSteven Spielberg\n\n\nInception\n2010\nAction\n8.8\nChristopher Nolan\n\n\nFight Club\n1999\nDrama\n8.8\nDavid Fincher\n\n\n\n\n\n\n\nTable 2, containing the same data, might look like this:\n\n\n\n\n\n\n\n\nRelease\nGenre\nIMDB_Rating\nDirector\nSeries_Title_Rating\n\n\n\n\n1994\nDrama\n9.3\nFrank Darabont\nThe Shawshank Redemption , 9.3\n\n\n1972\nCrime\n9.2\nFrancis Ford Coppola\nThe Godfather , 9.2\n\n\n2008\nAction\n9.0\nChristopher Nolan\nThe Dark Knight , 9\n\n\n1974\nCrime\n9.0\nFrancis Ford Coppola\nThe Godfather: Part II , 9\n\n\n1957\nCrime\n9.0\nSidney Lumet\n12 Angry Men , 9\n\n\n2003\nAction\n8.9\nPeter Jackson\nThe Lord of the Rings: The Return of the King , 8.9\n\n\n1994\nCrime\n8.9\nQuentin Tarantino\nPulp Fiction , 8.9\n\n\n1993\nBiography\n8.9\nSteven Spielberg\nSchindler's List , 8.9\n\n\n2010\nAction\n8.8\nChristopher Nolan\nInception , 8.8\n\n\n1999\nDrama\n8.8\nDavid Fincher\nFight Club , 8.8\n\n\n\n\n\n\n\nOr Table 3 could be structured like this:\n\n\n\n\n\n\n\n\nSeries\nDetails\n\n\n\n\nThe Shawshank Redemption\nFrank Darabont , Drama , 1994 , 9.3\n\n\nThe Godfather\nFrancis Ford Coppola , Crime , 1972 , 9.2\n\n\nThe Dark Knight\nChristopher Nolan , Action , 2008 , 9\n\n\nThe Godfather: Part II\nFrancis Ford Coppola , Crime , 1974 , 9\n\n\n12 Angry Men\nSidney Lumet , Crime , 1957 , 9\n\n\nThe Lord of the Rings: The Return of the King\nPeter Jackson , Action , 2003 , 8.9\n\n\nPulp Fiction\nQuentin Tarantino , Crime , 1994 , 8.9\n\n\nSchindler's List\nSteven Spielberg , Biography , 1993 , 8.9\n\n\nInception\nChristopher Nolan , Action , 2010 , 8.8\n\n\nFight Club\nDavid Fincher , Drama , 1999 , 8.8\n\n\n\n\n\n\n\nIn all these three instances, the data representation is perfectly valid but each one is harder to use in analysis (particularly the third table!)\nThe first table is the easiest to use as it follows the three tidy principles:\n\nEach column is a variable.\nEach row is an observation.\nEach cell (intersection between the column and row) is a SINGLE value.\n\n\n\n\nA tidy data representation\n\n\nThese simple principles allows for data to be stored consistently, and allows for R to leverage on its vectorised approach processing data, i.e., a tidy function can perform the same transformation along a variable consistently if the data within that variable is the same.\nFor example, converting the IMDB_rating variable in the first table to a percentage would be as simple as multiplying the variable by 10 (existing rating is out of 10):\n\n\n\n\n\n\n\n\nSeries_Title\nRelease\nGenre\nIMDB_Rating\nDirector\n\n\n\n\nThe Shawshank Redemption\n1994\nDrama\n93\nFrank Darabont\n\n\nThe Godfather\n1972\nCrime\n92\nFrancis Ford Coppola\n\n\nThe Dark Knight\n2008\nAction\n90\nChristopher Nolan\n\n\nThe Godfather: Part II\n1974\nCrime\n90\nFrancis Ford Coppola\n\n\n12 Angry Men\n1957\nCrime\n90\nSidney Lumet\n\n\nThe Lord of the Rings: The Return of the King\n2003\nAction\n89\nPeter Jackson\n\n\nPulp Fiction\n1994\nCrime\n89\nQuentin Tarantino\n\n\nSchindler's List\n1993\nBiography\n89\nSteven Spielberg\n\n\nInception\n2010\nAction\n88\nChristopher Nolan\n\n\nFight Club\n1999\nDrama\n88\nDavid Fincher"
  },
  {
    "objectID": "posts/tidy-data/index.html#a-simple-example",
    "href": "posts/tidy-data/index.html#a-simple-example",
    "title": "What is tidy data?",
    "section": "A Simple Example",
    "text": "A Simple Example\nLet‚Äôs take an example of how we might intuitively look at a dataset to determine if its ‚Äútidy-ness‚Äù. Consider the following hypothetical table that is tracking student records at a local school.\n\n\n\n\n\n\n\n\nName\nStudentID\nAge_and_Sex\nScience_Score\nMath_Score\nEnglish_Score\n\n\n\n\nRobert\nS001\n18 M\n90\n95\n70\n\n\nJohn\nS002\n15 M\n88\n79\n60\n\n\nJane\nS003\n17 F\n50\n50\n55\n\n\nAbdul\nS004\n14 M\n70\n90\n100\n\n\nTracy\nS005\n16 F\n40\n100\n90\n\n\n\n\n\n\n\nWhat do you notice? Based off the rules that we have, we have a couple of violations. Let‚Äôs reflect this against the Tidy Principles:\n1. Each column is a variable.\nAlthough each column could be considered a variable, the context is wanting. Although Name and StudentID satisfy this rule, we can clearly see that the ‚ÄúAge_and_Sex‚Äù variables seems to represent two variables fused as one (Age and Sex).\nAlso notice the variables for each subject score - Science_Score, Math_Score and English_Score. These variable names seem not to be variables but rather values masked as variables - Science, Math and English. In essence, these values should be put under a new variable with an appropriate variable name (e.g.¬†Subject) and the values - the scores - put within a second variable with an appropriate variable name (e.g.¬†Score).\n2. Each row is an observation.\nAlthough each row in essence is an observation, the placement of the values would need to change given the above rule violations. Implementing the above changes would reduce the ‚Äúlength‚Äù of each observation - the number of variables captured per observation would reduce from 6 per observation to just 4 (Name, StudentID, Subject, and Score).\n3. Each cell (intersection between the column and row) is a SINGLE value.\nIt was evident that the ‚ÄúAge_And_Sex‚Äù column had two values instead of just a single value.\nGiven the above principle violations, the ‚Äúuntidy‚Äù data would probably be best structured as follows in order satisfying each of the three Tidy Principles:\n\n\n\n\n\n\n\n\nName\nStudentID\nAge\nSex\nSubject\nScore\n\n\n\n\nRobert\nS001\n18\nM\nScience\n90\n\n\nRobert\nS001\n18\nM\nMath\n95\n\n\nRobert\nS001\n18\nM\nEnglish\n70\n\n\nJohn\nS002\n15\nM\nScience\n88\n\n\nJohn\nS002\n15\nM\nMath\n79\n\n\nJohn\nS002\n15\nM\nEnglish\n60\n\n\nJane\nS003\n17\nF\nScience\n50\n\n\nJane\nS003\n17\nF\nMath\n50\n\n\nJane\nS003\n17\nF\nEnglish\n55\n\n\nAbdul\nS004\n14\nM\nScience\n70\n\n\nAbdul\nS004\n14\nM\nMath\n90\n\n\nAbdul\nS004\n14\nM\nEnglish\n100\n\n\nTracy\nS005\n16\nF\nScience\n40\n\n\nTracy\nS005\n16\nF\nMath\n100\n\n\nTracy\nS005\n16\nF\nEnglish\n90"
  },
  {
    "objectID": "posts/tidy-data/index.html#conclusion",
    "href": "posts/tidy-data/index.html#conclusion",
    "title": "What is tidy data?",
    "section": "Conclusion",
    "text": "Conclusion\nFocusing on cleaning one‚Äôs data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one‚Äôs analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/okr-support-innovation/index.html",
    "href": "posts/okr-support-innovation/index.html",
    "title": "How can OKRs Support Innovation Creation and Discovery?",
    "section": "",
    "text": "A lot has been talked about when it comes to Objectives and Key Results (OKRs). Some see it as the ‚Äúholy grail‚Äù to improving organisational performance, whilst others see it as ‚Äújust another performance management tool‚Äù. But, as a mental model, it works well in driving daily performance expectations, projects, and innovation discovery and creation.\nSo, how can OKRs support innovation? Let‚Äôs first understand briefly what OKRs are all about.\nOKRs can be seen as a performance management tool, but one that focuses on goals as a means to help guide an organisation, team or individual align towards delivering measurable and impactful outcomes. As one would notice, the words in bold are those typically used as pillars of innovative teams and activities.\n\nObjectives state a concrete, action-oriented and inspirational desired state, whilst Key Results help define specific, time bound, aggressive but realistic, measurable and verifiable targets.\n\nLet‚Äôs put it into context.\nAs a typical example, consider a company that is embarking on making its way of doing business sustainable and environmentally friendly. As a consultancy company, it has recognised that its carbon footprint is quite high compared to other companies in its industry.\nIt therefore creates a set of OKRs that can help the organisation start its journey towards this goal.\n\n\n\n\n\n\n\nObjective\nCreate the lowest carbon footprint in the consultancy industry\n\n\n\n\nKey Result 1\nReduce the use of paper printing in its report development process by 90%.\n\n\nKey Result 2\nReduce the company‚Äôs energy use by 50%.\n\n\nKey Result 3\nUse 100% reusable or recyclable material for all client events.\n\n\n\nAs we can see, the Objective is concrete, action-oriented and inspirational, whilst its accompanying Key Results are specific and aggressive, but are measurable and verifiable. We can assume for this example that these are for the entire year (time bound).\nHow about innovation?\nNot all organisations are the same when it comes to managing innovation. In my opinion, innovation should be part of how organisations do work as opposed to a ‚Äúplug-in‚Äù where it comes in after the fact. Innovation should be embedded implicitly in the way organisations set out their strategy and goals, but rewarded explicitly to help drive the creative culture an organisation strives to have. A culture of innovation expedites the speed of an organisation in meeting its strategic goals and allows for new challenges to quickly come on-board.\nAs an example, let‚Äôs assume a Fintech is looking at adding Investments as a product line and sees building a savings culture with its customers as a potential strategy to increase its liquidity for investing. It therefore has ‚Äúmaking savings a way of life‚Äù as a company-wide objective and identifies 3 key results that will define its success:\n\n\n\n\n\n\n\nObjective\nMake savings a way of life for its customers\n\n\n\n\nKey Result 1\nMake its saving product the best performing product within 2 years in its portfolio.\n\n\nKey Result 2\nMake enrolment and use of the product as seamless as possible.\n\n\nKey Result 3\nAt least 40% of its customer base should be enrolled onto the product by the end of the financial year.\n\n\n\nThe company‚Äôs product and technology teams for this company decide to pick up on this new objective and create new team objectives that feed into Key Results 1 and 2 respectively.\nThe product team may leverage on its design thinking approach to craft a set of key results that can enable the team identify a potentially new product for the company:\n\n\n\n\n\n\n\nObjective\nCreate a saving product that is the best offering in the company‚Äôs portfolio\n\n\n\n\nKey Result 1\nTalk to 80 new and 50 existing customers by the end of January.\n\n\nKey Result 2\nPropose (at most) 3 pain points that could be solved by savings by the end of January.\n\n\nKey Result 3\nTest at least one high-fidelity potential prototype with at least 80 customers by the end of March.\n\n\nKey Result 4\nPresent a viable product for scaling by 15 April.\n\n\n\nThe technology team for this company may craft OKRs aligned to the Key Result 2 as follows:\n\n\n\n\n\n\n\nObjective\nBuild technology that enables a seamless customer saving experience by end of Q4\n\n\n\n\nKey Result 1\nLaunch mobile app by by end of Q3.\n\n\nKey Result 2\nEnable real-time reporting on product adoption and usage by Q3.\n\n\nKey Result 3\nScale infrastructure to support 1,000,000 customers by the end of Q4.\n\n\nKey Result 4\nEnsure 100% compliance to GDPR and related data protection laws.\n\n\n\nThese Key Results may be further broken down by teams within the technology department that may be in charge of software development, infrastructure support, information security and others.\nThis is where the alignment of OKRs come into fruition - a pyramid of objectives and key results can be created across teams, each feeding into the other to ensure alignment to the overall strategic goal.\nOKRs define success and set ‚Äúwhat‚Äù needs to be accomplished, not ‚Äúhow‚Äù it should be accomplished. This allows for teams to create initiatives that can meet the aggressive key results, the beauty being that teams will need to think differently and out-of-the-box to create really innovative solutions that can rise to these hard targets.\nIf implemented well, OKRs could be the answer to many organisations looking to embed innovation as part of its culture. The only thing that remains is creating an ecosystem of knowledge and partners, and a recognition and rewards system that can empower, inspire and allow staff to try, fail, learn and innovate to their best!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
<<<<<<< HEAD
    "text": "Data Cleaning Series :: Exploring Social Media Addiction Data\n\n\n\n\n\n\ndata-cleaning\n\n\ndata-exploration\n\n\n\n\n\n\n\n\n\nJul 22, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nRegex-ing in R\n\n\n\n\n\n\ndata-cleaning\n\n\n\n\n\n\n\n\n\nMay 30, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is tidy data?\n\n\n\n\n\n\ndata-cleaning\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nHow can OKRs Support Innovation Creation and Discovery?\n\n\n\n\n\n\nperformance\n\n\ninnovation\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\nNo matching items"
=======
    "text": "What is tidy data?\n\n\n\n\n\n\ndata-cleaning\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\n\n\n\n\n\n\nHow can OKRs Support Innovation Creation and Discovery?\n\n\n\n\n\n\nperformance\n\n\ninnovation\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nRobert Muwanga\n\n\n\n\n\n\nNo matching items"
>>>>>>> 5f5e500f48b3ec108deae015b895118b9d0c4125
  },
  {
    "objectID": "posts/intro-regex/index.html",
    "href": "posts/intro-regex/index.html",
    "title": "Regex-ing in R",
    "section": "",
    "text": "Regular Expressions, often shortened as Regex, is a powerful concept in modern programming languages for manipulating strings. A single line of code implementing a regex pattern can save several lines of code, keeping the overall codebase concise and easily maintainable."
  },
  {
    "objectID": "posts/intro-regex/index.html#what-is-regex",
    "href": "posts/intro-regex/index.html#what-is-regex",
    "title": "Regex-ing in R",
    "section": "",
    "text": "Regular Expressions, often shortened as Regex, is a powerful concept in modern programming languages for manipulating strings. A single line of code implementing a regex pattern can save several lines of code, keeping the overall codebase concise and easily maintainable."
  },
  {
    "objectID": "posts/intro-regex/index.html#so-what-does-it-do",
    "href": "posts/intro-regex/index.html#so-what-does-it-do",
    "title": "Regex-ing in R",
    "section": "So what does it do?",
<<<<<<< HEAD
    "text": "So what does it do?\nIt uses pattern matching to find [TBD]‚Ä¶"
  },
  {
    "objectID": "posts/intro-regex/index.html#title",
    "href": "posts/intro-regex/index.html#title",
    "title": "Regex-ing in R",
    "section": "[Title]",
    "text": "[Title]\nTBD"
=======
    "text": "So what does it do?\nIt uses pattern matching to find‚Ä¶"
>>>>>>> 5f5e500f48b3ec108deae015b895118b9d0c4125
  },
  {
    "objectID": "posts/intro-regex/index.html#conclusion",
    "href": "posts/intro-regex/index.html#conclusion",
    "title": "Regex-ing in R",
    "section": "Conclusion",
<<<<<<< HEAD
    "text": "Conclusion"
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html",
    "href": "posts/exploratory-analysis-social-media/index.html",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "",
    "text": "I was asked to teach a class on how to import, clean and transform data in R and as part of the training. We did several exercises to help re-enforce the basics and, given that it was well received, I decided to document these exercises in a series that I‚Äôm calling the Data Cleaning Series.\nIt will also serve as a journal for me to refer to when (being human) I forget üòÖ.\nIf you haven‚Äôt done so yet I recommend you read my brief entry on tidy data as we shall endeavour to put our data in this format throughout the series."
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html#background-to-the-data-cleaning-series",
    "href": "posts/exploratory-analysis-social-media/index.html#background-to-the-data-cleaning-series",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "",
    "text": "I was asked to teach a class on how to import, clean and transform data in R and as part of the training. We did several exercises to help re-enforce the basics and, given that it was well received, I decided to document these exercises in a series that I‚Äôm calling the Data Cleaning Series.\nIt will also serve as a journal for me to refer to when (being human) I forget üòÖ.\nIf you haven‚Äôt done so yet I recommend you read my brief entry on tidy data as we shall endeavour to put our data in this format throughout the series."
  },
  {
    "objectID": "posts/exploratory-analysis-social-media/index.html#about-the-dataset---social-media-addiction-analysis",
    "href": "posts/exploratory-analysis-social-media/index.html#about-the-dataset---social-media-addiction-analysis",
    "title": "Data Cleaning Series :: Exploring Social Media Addiction Data",
    "section": "About the dataset - Social Media Addiction Analysis",
    "text": "About the dataset - Social Media Addiction Analysis\nFor this exercise, we are leveraging on the Social Media Addiction dataset provided by Anil Shamim from the Kaggle Platform.\nThe data set is a set of anonymized records of students‚Äô social media behaviors and related life outcomes, spanning multiple countries and background. Each row represents one student‚Äôs survey response, offering a cross‚Äêsectional snapshot suitable for statistical analysis and machine‚Äêlearning applications.\nFor full details of the dataset, I would encourage you to visit the site and view its Data Card.\nMake sure you download the data set and save it on your machine (you might need a free Kaggle account to get it).\nLet‚Äôs import it in and do a bit of exploration to get to know it better:\n\nsocial_media_data &lt;- read_csv('Students_Social_Media_Addiction.csv')\n\nRows: 705 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (6): Gender, Academic_Level, Country, Most_Used_Platform, Affects_Academ...\ndbl (7): Student_ID, Age, Avg_Daily_Usage_Hours, Sleep_Hours_Per_Night, Ment...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsocial_media_data\n\n# A tibble: 705 √ó 13\n   Student_ID   Age Gender Academic_Level Country     Avg_Daily_Usage_Hours\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;                       &lt;dbl&gt;\n 1          1    19 Female Undergraduate  Bangladesh                    5.2\n 2          2    22 Male   Graduate       India                         2.1\n 3          3    20 Female Undergraduate  USA                           6  \n 4          4    18 Male   High School    UK                            3  \n 5          5    21 Male   Graduate       Canada                        4.5\n 6          6    19 Female Undergraduate  Australia                     7.2\n 7          7    23 Male   Graduate       Germany                       1.5\n 8          8    20 Female Undergraduate  Brazil                        5.8\n 9          9    18 Male   High School    Japan                         4  \n10         10    21 Female Graduate       South Korea                   3.3\n# ‚Ñπ 695 more rows\n# ‚Ñπ 7 more variables: Most_Used_Platform &lt;chr&gt;,\n#   Affects_Academic_Performance &lt;chr&gt;, Sleep_Hours_Per_Night &lt;dbl&gt;,\n#   Mental_Health_Score &lt;dbl&gt;, Relationship_Status &lt;chr&gt;,\n#   Conflicts_Over_Social_Media &lt;dbl&gt;, Addicted_Score &lt;dbl&gt;\n\n\nFrom the output, we notice that the dataset has 705 variables and 13 observations. Given that we have the Data Caard that outlines the expected values for each variable, we can validate the information to make sure that it conforms to what was expected:\n\n# Let's build the validation engine based on the rules on the Data Card.\nvalidation_rules &lt;- validator(\n  is_unique(Student_ID), \n  Age &gt; 0, \n  Gender %in% c(\"Male\", \"Female\"), \n  Academic_Level %in% c(\"High School\", \"Undergraduate\", \"Graduate\"), \n  in_range(Avg_Daily_Usage_Hours, 0, 24),\n  Affects_Academic_Performance %in% c(\"Yes\", \"No\"),\n  Sleep_Hours_Per_Night &gt; 0,\n  in_range(Mental_Health_Score, 1, 10),\n  Relationship_Status %in% c(\"Single\", \"In Relationship\", \"Complicated\"),\n  Conflicts_Over_Social_Media &gt;= 0,\n  in_range(Addicted_Score, 1, 10),\n  is.character(Country), \n  is.character(Most_Used_Platform)\n)\n\n# Let's validate and assess the output.\nconfront(social_media_data, validation_rules)\n\nObject of class 'validation'\nCall:\n    confront(dat = social_media_data, x = validation_rules)\n\nRules confronted: 13\n   With fails   : 0\n   With missings: 0\n   Threw warning: 0\n   Threw error  : 0\n\n\nFrom the output, it is clear that all the variables satisfy the set constraints. However, as a sanity check for Country and Most_Used_Platform, let‚Äôs print out their unique values to see whether they are a good representation of what currently exists:\n\n# Let's just view the unique value of the Country variable\n\nsocial_media_data |&gt; \n  select(Country) |&gt; \n  unique() |&gt; \n  pull()\n\n  [1] \"Bangladesh\"      \"India\"           \"USA\"             \"UK\"             \n  [5] \"Canada\"          \"Australia\"       \"Germany\"         \"Brazil\"         \n  [9] \"Japan\"           \"South Korea\"     \"France\"          \"Spain\"          \n [13] \"Italy\"           \"Mexico\"          \"Russia\"          \"China\"          \n [17] \"Sweden\"          \"Norway\"          \"Denmark\"         \"Netherlands\"    \n [21] \"Belgium\"         \"Switzerland\"     \"Austria\"         \"Portugal\"       \n [25] \"Greece\"          \"Ireland\"         \"New Zealand\"     \"Singapore\"      \n [29] \"Malaysia\"        \"Thailand\"        \"Vietnam\"         \"Philippines\"    \n [33] \"Indonesia\"       \"Taiwan\"          \"Hong Kong\"       \"Turkey\"         \n [37] \"Israel\"          \"UAE\"             \"Egypt\"           \"Morocco\"        \n [41] \"South Africa\"    \"Nigeria\"         \"Kenya\"           \"Ghana\"          \n [45] \"Argentina\"       \"Chile\"           \"Colombia\"        \"Peru\"           \n [49] \"Venezuela\"       \"Ecuador\"         \"Uruguay\"         \"Paraguay\"       \n [53] \"Bolivia\"         \"Costa Rica\"      \"Panama\"          \"Jamaica\"        \n [57] \"Trinidad\"        \"Bahamas\"         \"Iceland\"         \"Finland\"        \n [61] \"Poland\"          \"Romania\"         \"Hungary\"         \"Czech Republic\" \n [65] \"Slovakia\"        \"Croatia\"         \"Serbia\"          \"Slovenia\"       \n [69] \"Bulgaria\"        \"Estonia\"         \"Latvia\"          \"Lithuania\"      \n [73] \"Ukraine\"         \"Moldova\"         \"Belarus\"         \"Kazakhstan\"     \n [77] \"Uzbekistan\"      \"Kyrgyzstan\"      \"Tajikistan\"      \"Armenia\"        \n [81] \"Georgia\"         \"Azerbaijan\"      \"Cyprus\"          \"Malta\"          \n [85] \"Luxembourg\"      \"Monaco\"          \"Andorra\"         \"San Marino\"     \n [89] \"Vatican City\"    \"Liechtenstein\"   \"Montenegro\"      \"Albania\"        \n [93] \"North Macedonia\" \"Kosovo\"          \"Bosnia\"          \"Qatar\"          \n [97] \"Kuwait\"          \"Bahrain\"         \"Oman\"            \"Jordan\"         \n[101] \"Lebanon\"         \"Iraq\"            \"Yemen\"           \"Syria\"          \n[105] \"Afghanistan\"     \"Pakistan\"        \"Nepal\"           \"Bhutan\"         \n[109] \"Sri Lanka\"       \"Maldives\"       \n\n\n\n# Let's just view the unique values of Most_Used_Platform variable\n\nsocial_media_data |&gt; \n  select(Most_Used_Platform) |&gt; \n  unique() |&gt; \n  pull()\n\n [1] \"Instagram\" \"Twitter\"   \"TikTok\"    \"YouTube\"   \"Facebook\"  \"LinkedIn\" \n [7] \"Snapchat\"  \"LINE\"      \"KakaoTalk\" \"VKontakte\" \"WhatsApp\"  \"WeChat\"   \n\n\nWe can see that both outputs seem reasonable. Additional data cleaning may be needed but we shall further clean as part of answering the questions.\nLet‚Äôs proceed to answering the questions!\n\nQuestion 1\n\nWhat is the total number of male and female students by country?\n\nsocial_media_data |&gt; \n  summarise(total = n(), .by = Gender) |&gt; \n  mutate(\n    perc_gender = round(\n      total / sum(total) * 100, 2))\n\n# A tibble: 2 √ó 3\n  Gender total perc_gender\n  &lt;chr&gt;  &lt;int&gt;       &lt;dbl&gt;\n1 Female   353        50.1\n2 Male     352        49.9\n\n\nWe can see that its an almost equal split between the Female and the Male gender - 50.07% against 49.93% respectively.\n\n\n\nQuestion 2\n\nWhich social media platform is associated with affecting the largest number of students by Academic Level?\n\n\n\nQuestion 3\n\nShow a table of countries showing the average sleep of High School students ordered from the least amount of sleep to the most.\n\n\n\nQuestion 4\n\nFind the average addiction score for each country. Include the country‚Äôs respective continent and order the table in descending order by the country‚Äôs average addiction score\n\n\n\nQuestion 5\n\nWhich gender per country and continent is the most lonely? Assume ‚Äúbeing lonely‚Äù are those in a relationship status of ‚ÄúSingle‚Äù and ‚ÄúComplicated‚Äù.\n\n\n\nQuestion 6\n\nWhich capital city (and country) has the most number of students ‚ÄúIn Relationship‚Äù?\n\n\n\nQuestion 7\n\nDetermine the most popular social media platform in each country and continent"
=======
    "text": "Conclusion\nFocusing on cleaning one‚Äôs data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one‚Äôs analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
  },
  {
    "objectID": "posts/tt-2022-41/index.html",
    "href": "posts/tt-2022-41/index.html",
    "title": "TidyTuesday - 2022 Week 41 [Ravelry Yarn]",
    "section": "",
    "text": "This week‚Äôs tidytuesday dataset is provided by Alice Walsh who collected data from ravelry.com.\nRavelry is a free website for knitters, crocheters and fiber artists. The site hosts millions of yarn lovers from all over the world and provide personal notebooks for fiber artists to keep track of their projects, yarn and fibers, tools, and pattern library."
  },
  {
    "objectID": "posts/tt-2022-41/index.html#ravelry-yarn",
    "href": "posts/tt-2022-41/index.html#ravelry-yarn",
    "title": "TidyTuesday - 2022 Week 41 [Ravelry Yarn]",
    "section": "",
    "text": "This week‚Äôs tidytuesday dataset is provided by Alice Walsh who collected data from ravelry.com.\nRavelry is a free website for knitters, crocheters and fiber artists. The site hosts millions of yarn lovers from all over the world and provide personal notebooks for fiber artists to keep track of their projects, yarn and fibers, tools, and pattern library."
  },
  {
    "objectID": "posts/tt-2022-41/index.html#about-the-data",
    "href": "posts/tt-2022-41/index.html#about-the-data",
    "title": "TidyTuesday - 2022 Week 41 [Ravelry Yarn]",
    "section": "About the data",
    "text": "About the data\n\nyarn &lt;- tuesdata$yarn"
  },
  {
    "objectID": "posts/tt-2022-41/index.html#conclusion",
    "href": "posts/tt-2022-41/index.html#conclusion",
    "title": "TidyTuesday - 2022 Week 41 [Ravelry Yarn]",
    "section": "Conclusion",
    "text": "Conclusion\nFocusing on cleaning one‚Äôs data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one‚Äôs analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
  },
  {
    "objectID": "posts/tt-2021-29/index.html",
    "href": "posts/tt-2021-29/index.html",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "",
    "text": "This week‚Äôs tidytuesday dataset is provided by plummye who manually aggregated the Scooby Doo episode data, watching every Scooby-Doo iteration and tracking every variable."
  },
  {
    "objectID": "posts/tt-2021-29/index.html#scooby-doo-episodes",
    "href": "posts/tt-2021-29/index.html#scooby-doo-episodes",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "",
    "text": "This week‚Äôs tidytuesday dataset is provided by plummye who manually aggregated the Scooby Doo episode data, watching every Scooby-Doo iteration and tracking every variable."
  },
  {
    "objectID": "posts/tt-2021-29/index.html#about-the-data",
    "href": "posts/tt-2021-29/index.html#about-the-data",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "About the data",
    "text": "About the data\nThe has 603 observations across 75 variables, running from 1969-09-13 to 2021-02-25.\n\nscooby &lt;- tuesdata$scoobydoo\nscooby\n\n# A tibble: 603 √ó 75\n   index series_name   network season title imdb  engagement date_aired run_time\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;date&gt;        &lt;dbl&gt;\n 1     1 Scooby Doo, ‚Ä¶ CBS     1      What‚Ä¶ 8.1   556        1969-09-13       21\n 2     2 Scooby Doo, ‚Ä¶ CBS     1      A Cl‚Ä¶ 8.1   479        1969-09-20       22\n 3     3 Scooby Doo, ‚Ä¶ CBS     1      Hass‚Ä¶ 8     455        1969-09-27       21\n 4     4 Scooby Doo, ‚Ä¶ CBS     1      Mine‚Ä¶ 7.8   426        1969-10-04       21\n 5     5 Scooby Doo, ‚Ä¶ CBS     1      Deco‚Ä¶ 7.5   391        1969-10-11       21\n 6     6 Scooby Doo, ‚Ä¶ CBS     1      What‚Ä¶ 8.4   384        1969-10-18       21\n 7     7 Scooby Doo, ‚Ä¶ CBS     1      Neve‚Ä¶ 7.6   358        1969-10-25       21\n 8     8 Scooby Doo, ‚Ä¶ CBS     1      Foul‚Ä¶ 8.2   358        1969-11-01       21\n 9     9 Scooby Doo, ‚Ä¶ CBS     1      The ‚Ä¶ 8.1   371        1969-11-08       21\n10    10 Scooby Doo, ‚Ä¶ CBS     1      Bedl‚Ä¶ 8     346        1969-11-15       21\n# ‚Ñπ 593 more rows\n# ‚Ñπ 66 more variables: format &lt;chr&gt;, monster_name &lt;chr&gt;, monster_gender &lt;chr&gt;,\n#   monster_type &lt;chr&gt;, monster_subtype &lt;chr&gt;, monster_species &lt;chr&gt;,\n#   monster_real &lt;chr&gt;, monster_amount &lt;dbl&gt;, caught_fred &lt;chr&gt;,\n#   caught_daphnie &lt;chr&gt;, caught_velma &lt;chr&gt;, caught_shaggy &lt;chr&gt;,\n#   caught_scooby &lt;chr&gt;, captured_fred &lt;chr&gt;, captured_daphnie &lt;chr&gt;,\n#   captured_velma &lt;chr&gt;, captured_shaggy &lt;chr&gt;, captured_scooby &lt;chr&gt;, ‚Ä¶"
  },
  {
    "objectID": "posts/tt-2021-29/index.html#conclusion",
    "href": "posts/tt-2021-29/index.html#conclusion",
    "title": "TidyTuesday - 2021 Week 29 [Scooby Doo]",
    "section": "Conclusion",
    "text": "Conclusion\nFocusing on cleaning one‚Äôs data set into a tidy dataset helps to set one up for success, making it easier to use functions that comply with the tidy principles in one‚Äôs analysis. It may take a bit of work to get it right but once its done appropriately, it is quite powerful."
>>>>>>> 5f5e500f48b3ec108deae015b895118b9d0c4125
  }
]